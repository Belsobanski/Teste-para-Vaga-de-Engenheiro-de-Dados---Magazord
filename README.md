# Teste para Vaga de Engenheiro de Dados 

Link do teste: Teste - Data Engineer üöÄ  https://github.com/Belsobanski/data_engineer_test/blob/master/README.md

## Data Lake Simulation

Este reposit√≥rio simula um **Data Lake** com tr√™s camadas principais: **raw_layer**, **process** e **trusted**. O objetivo do projeto √© demonstrar como construir um pipeline de ingest√£o e transforma√ß√£o de dados usando scripts Python para manipula√ß√£o de arquivos CSV e carregamento em um banco de dados.

## Estrutura do Data Lake

### **raw_layer**:
Cont√©m os dados brutos fornecidos pela atividade.

### **process**:
Cont√©m os dados processados, ou seja, os dados das tabelas "clientes", "produtos" e "transacoes" ap√≥s passarem por um processo de limpeza e transforma√ß√£o.

### **trusted**:
Cont√©m os dados agregados para an√°lises mais perform√°ticas:
- Receita total por cliente.
- N√∫mero total de transa√ß√µes por cliente.
- Produto mais comprado por cliente.

## Tecnologias Utilizadas

As tecnologias utilizadas nesse projeto incluem:

- **PostgreSQL**: Banco de dados relacional utilizado para armazenar dados limpos e otimizados no DataMart.
- **SQL**: Linguagem de consulta usada para criar e executar consultas anal√≠ticas.
- **Python**: Utilizado para a cria√ß√£o de pipelines e scripts de transforma√ß√£o de dados.
- **GitHub**: Plataforma para versionamento de c√≥digo, incluindo o gerenciamento do c√≥digo-fonte e das partes do arquivo de banco de dados.
- **Git Bash**: Ferramenta de linha de comando utilizada para manuseio e manipula√ß√£o de arquivos grandes, como o uso de comandos `split` e `cat` para dividir e unir arquivos.


## Descri√ß√£o dos Scripts

### **1. Scripts de Limpeza e Transforma√ß√£o (camada `process`)**

Esses scripts s√£o respons√°veis pela transforma√ß√£o dos dados das tabelas "clientes", "produtos" e "transacoes". Cada tabela √© processada de forma independente, o que facilita o controle e manuten√ß√£o do c√≥digo.

#### Exemplo do script `limpar_transformacao_nome_da_tabela.py`:

1. **Importa√ß√£o das bibliotecas**:
   - `pandas`: Para manipula√ß√£o dos dados.
   - `os`: Para manipula√ß√£o de diret√≥rios.
   - `glob`: Para buscar arquivos dentro do diret√≥rio.

2. **Defini√ß√£o dos Caminhos**:
   - `arquivo_entrada`: Define a localiza√ß√£o dos arquivos CSV brutos.
   - `diretorio_saida`: Define onde o arquivo processado ser√° salvo.

3. **Carregamento dos Arquivos CSV**:
   - L√™ e combina os arquivos CSV em um √∫nico DataFrame usando `glob` e `pandas`.

4. **Normaliza√ß√£o dos Dados**:
   - Padroniza os nomes das colunas (para min√∫sculas e remove espa√ßos extras).
   - Converte tipos de dados, como IDs e quantidades para inteiros.
   - Trata a coluna de data para garantir o formato correto.

5. **Organiza√ß√£o e Exporta√ß√£o**:
   - Ordena os dados e realiza diversas transforma√ß√µes, como remo√ß√£o de duplicatas e preenchimento de valores nulos.
   - Salva os dados transformados em um novo arquivo CSV.

### **2. Scripts de Cria√ß√£o e Carregamento no Banco de Dados**

Ap√≥s a limpeza e transforma√ß√£o dos dados, os dados s√£o carregados no banco de dados "datamart" para serem usados em an√°lises e relat√≥rios.

#### Exemplo do script `criar_carregar_nome_da_tabela.py`:

1. **Importa√ß√£o das Bibliotecas**:
   - `pandas`: Para manipula√ß√£o dos dados.
   - `sqlalchemy`: Para conex√£o com o banco de dados e inser√ß√£o de dados.
   - `psycopg2`: Para conex√£o com PostgreSQL.

2. **Direcionamento dos Dados**:
   - Define o caminho para os dados processados.

3. **Conex√£o com o Banco de Dados**:
   - Conecta-se ao banco de dados "datamart" usando `SQLAlchemy`.

4. **Carregamento dos Dados**:
   - Carrega os dados processados e insere-os no banco de dados usando `pandas.DataFrame.to_sql`.

5. **Finaliza√ß√£o**:
   - Exibe uma mensagem de sucesso ao final do carregamento.

### **3. Script de Cria√ß√£o e Agrega√ß√£o dos Dados - `criar_carregar_tabela_agregada_postgres.py`**

#### Objetivo:
Criar uma nova tabela agregada com:
- **Receita total por cliente**
- **N√∫mero total de transa√ß√µes por cliente**
- **Produto mais comprado por cliente**

Carregar essa tabela agregada no banco de dados **datamart** e salvar os dados no formato **Parquet** na camada **trusted** para an√°lises mais perform√°ticas.

#### Explica√ß√£o do C√≥digo:

1. **Importa√ß√£o das Bibliotecas**:
   - `pandas`: Para manipula√ß√£o dos dados.
   - `sqlalchemy`: Para conectar ao banco de dados PostgreSQL.
   - `psycopg2`: Para a conex√£o com PostgreSQL.

2. **Carregamento dos Dados**:
   - Os dados processados das tabelas `clientes`, `transacoes` e `produtos` s√£o carregados em DataFrames do `pandas`.

3. **Agrega√ß√£o dos Dados**:
   - **Receita total por cliente**: Somamos o valor das transa√ß√µes por `cliente_id`.
   - **N√∫mero total de transa√ß√µes por cliente**: Contamos o n√∫mero de transa√ß√µes por `cliente_id`.
   - **Produto mais comprado por cliente**: Contamos a quantidade de cada produto comprado por `cliente_id` e escolhemos o produto com a maior quantidade comprada.

4. **Mesclagem dos DataFrames**:
   - As tabelas agregadas s√£o mescladas em um √∫nico DataFrame, utilizando o `cliente_id` como chave de jun√ß√£o.

5. **Conex√£o com o Banco de Dados**:
   - Utilizamos `SQLAlchemy` para conectar ao banco de dados **datamart** e carregamos os dados agregados na tabela **clientes_aggregados**.

6. **Salvar os Dados no Banco de Dados**:
   - O m√©todo `to_sql()` do `pandas` √© utilizado para criar a tabela no banco de dados e inserir os dados.

7. **Salvar os Dados no Formato Parquet**:
   - Os dados agregados s√£o salvos no formato **Parquet** na camada **trusted** para garantir uma an√°lise mais perform√°tica em etapas futuras.

8. **Mensagem de Conclus√£o**:
   - Ap√≥s a execu√ß√£o, o script exibe uma mensagem confirmando a cria√ß√£o da tabela no banco e o arquivo Parquet.

Toda a valida√ß√£o do pipeline de popular o banco de dados com dados limpos e normalizados foi realizada utilizando o script `teste.py`. Esse script foi utilizado para confirmar se os dados estavam sendo transformados corretamente antes de serem salvos no Data Lake e no PostgreSQL. 

A valida√ß√£o envolveu garantir que as opera√ß√µes de transforma√ß√£o e limpeza estavam sendo aplicadas corretamente aos dados antes da sua carga nos sistemas de armazenamento. O processo de valida√ß√£o foi fundamental para assegurar que os dados no Data Lake e no banco de dados PostgreSQL estivessem consistentes, precisos e prontos para uso em an√°lises e consultas.

## Banco de Dados DataMart no PostgreSQL

Este banco de dados foi criado para armazenar os dados limpos, normalizados e deduplicados provenientes de diversas fontes, como transa√ß√µes de vendas, clientes e produtos. O objetivo √© oferecer uma estrutura eficiente para an√°lise de dados e tomada de decis√µes.

### Estrutura do Banco de Dados

O banco de dados DataMart cont√©m as seguintes tabelas:

- **clientes**: Cont√©m as informa√ß√µes sobre os clientes, como `id_cliente`, `nome_cliente`, `email`, `telefone`.
- **produtos**: Armazena dados sobre os produtos, incluindo `id_produto`, `nome_produto`, `categoria`, `preco`.
- **transacoes**: Registra as transa√ß√µes realizadas pelos clientes, com campos como `id_transacao`, `id_cliente`, `id_produto`, `quantidade`, `data_transacao`.

As tabelas foram populadas atrav√©s de pipelines com os scripts:
- **limpar_transformacao_nome_da_tabela.py**: Respons√°vel pela limpeza, normaliza√ß√£o e deduplica√ß√£o dos dados antes de inseri-los nas tabelas.
- **criar_carregar_nome_da_tabela.py**: Respons√°vel por carregar os dados limpos nas tabelas correspondentes do banco de dados.

Al√©m disso, uma tabela agregada foi criada com os seguintes dados:
- **Receita total por cliente**
- **N√∫mero total de transa√ß√µes por cliente**
- **Produto mais comprado por cliente**

Essas informa√ß√µes s√£o salvas tamb√©m no DataLake, na camada **Trust**, garantindo que os dados mais confi√°veis estejam dispon√≠veis para futuras an√°lises.

## Melhorias de Performance

Para otimizar a performance do banco de dados e garantir consultas r√°pidas, foram criados os seguintes √≠ndices nas tabelas:

```sql
CREATE INDEX idx_clientes_id ON clientes(id);
CREATE INDEX idx_clientes_nome_sobrenome ON clientes(nome_cliente);
CREATE INDEX idx_produtos_id ON produtos(id_produto);
CREATE INDEX idx_produtos_descricao ON produtos(nome_produto);
CREATE INDEX idx_transacoes_id_cliente ON transacoes(id_cliente);
CREATE INDEX idx_transacoes_id_produto ON transacoes(id_produto);
CREATE INDEX idx_transacoes_data_transacao ON transacoes(data_transacao);

No projeto de Data Lake e DataMart descrito, a combina√ß√£o de
√≠ndices eficientes, particionamento de tabelas, consultas agregadas otimizadas,
uso de tabelas materializadas e an√°lise de planos de execu√ß√£o √© fundamental para garantir que
o sistema seja capaz de lidar com grandes volumes de dados de maneira eficiente e r√°pida.
A implementa√ß√£o dessas boas pr√°ticas no PostgreSQL** pode resultar em uma performance
significativamente melhorada, mesmo √† medida que os dados aumentam.

Para facilitar a obten√ß√£o de informa√ß√µes r√°pidas e importantes para as regras de neg√≥cio, algumas consultas SQL foram criadas:

### N√∫mero de clientes ativos (√∫ltimos 3 meses):

```sql
SELECT 
    COUNT(DISTINCT t.id_cliente) AS clientes_ativos
FROM 
    transacoes t
WHERE 
    t.data_transacao >= CURRENT_DATE - INTERVAL '3 months';

### Receita total por produto:

```sql
SELECT p.id_produto, SUM(t.quantidade * p.preco) AS receita_total
FROM transacoes t
JOIN produtos p ON t.id_produto = p.id_produto
GROUP BY p.id_produto
ORDER BY receita_total DESC;

### Top 5 produtos mais vendidos (ano de 2024):

```sql
SELECT p.nome_produto, SUM(t.quantidade) AS total_vendido
FROM transacoes t
JOIN produtos p ON t.id_produto = p.id_produto
WHERE t.data_transacao BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY p.nome_produto
ORDER BY total_vendido DESC
LIMIT 5;


Essas consultas foram projetadas para fornecer informa√ß√µes r√°pidas e detalhadas sobre o desempenho de vendas, clientes ativos e os produtos mais vendidos, ajudando a orientar as decis√µes de neg√≥cio.

##Conclus√£o

O banco de dados DataMart no PostgreSQL foi estruturado para oferecer dados limpos e otimizados, com foco na efici√™ncia e na rapidez de acesso. As opera√ß√µes de transforma√ß√£o e carga foram automatizadas com pipelines, e as consultas anal√≠ticas fornecem insights valiosos para a empresa.

O arquivo do banco de dados foi particionado devido ao seu tamanho total de 45.233kB, o que dificultaria o upload no GitHub. Para contornar essa limita√ß√£o, foi utilizado o m√©todo `split` para dividir o arquivo em 3 partes:

- `parte_1aa`
- `parte_1ab`
- `parte_1ac`

O comando utilizado para dividir o arquivo foi:

```bash
split -b 22m seu_arquivo.sql parte_1

Para unir as partes do arquivo de volta, foi utilizado o comando:

```bash
cat parte_1aa parte_1ab parte_1ac > arquivo_completo.sql

##Esse processo foi realizado utilizando o Git Bash para facilitar o manuseio dos arquivos grandes, garantindo que o arquivo completo fosse reconstru√≠do e pronto para ser utilizado.





