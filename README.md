# Teste para Vaga de Engenheiro de Dados 

Link do teste: Teste - Data Engineer ðŸš€  https://github.com/Belsobanski/data_engineer_test/blob/master/README.md

## Data Lake Simulation

Este repositÃ³rio simula um **Data Lake** com trÃªs camadas principais: **raw_layer**, **process** e **trusted**. O objetivo do projeto Ã© demonstrar como construir um pipeline de ingestÃ£o e transformaÃ§Ã£o de dados usando scripts Python para manipulaÃ§Ã£o de arquivos CSV e carregamento em um banco de dados.

## Estrutura do Data Lake

### **raw_layer**:
ContÃ©m os dados brutos fornecidos pela atividade.

### **process**:
ContÃ©m os dados processados, ou seja, os dados das tabelas "clientes", "produtos" e "transacoes" apÃ³s passarem por um processo de limpeza e transformaÃ§Ã£o.

### **trusted**:
ContÃ©m os dados agregados para anÃ¡lises mais performÃ¡ticas:
- Receita total por cliente.
- NÃºmero total de transaÃ§Ãµes por cliente.
- Produto mais comprado por cliente.

## Tecnologias Utilizadas

As tecnologias utilizadas nesse projeto incluem:

- **PostgreSQL**: Banco de dados relacional utilizado para armazenar dados limpos e otimizados no DataMart.
- **SQL**: Linguagem de consulta usada para criar e executar consultas analÃ­ticas.
- **Python**: Utilizado para a criaÃ§Ã£o de pipelines e scripts de transformaÃ§Ã£o de dados.
- **GitHub**: Plataforma para versionamento de cÃ³digo, incluindo o gerenciamento do cÃ³digo-fonte e das partes do arquivo de banco de dados.
- **Git Bash**: Ferramenta de linha de comando utilizada para manuseio e manipulaÃ§Ã£o de arquivos grandes, como o uso de comandos `split` e `cat` para dividir e unir arquivos.


## DescriÃ§Ã£o dos Scripts

### **1. Scripts de Limpeza e TransformaÃ§Ã£o (camada `process`)**

Esses scripts sÃ£o responsÃ¡veis pela transformaÃ§Ã£o dos dados das tabelas "clientes", "produtos" e "transacoes". Cada tabela Ã© processada de forma independente, o que facilita o controle e manutenÃ§Ã£o do cÃ³digo.

#### Exemplo do script `limpar_transformacao_nome_da_tabela.py`:

1. **ImportaÃ§Ã£o das bibliotecas**:
   - `pandas`: Para manipulaÃ§Ã£o dos dados.
   - `os`: Para manipulaÃ§Ã£o de diretÃ³rios.
   - `glob`: Para buscar arquivos dentro do diretÃ³rio.

2. **DefiniÃ§Ã£o dos Caminhos**:
   - `arquivo_entrada`: Define a localizaÃ§Ã£o dos arquivos CSV brutos.
   - `diretorio_saida`: Define onde o arquivo processado serÃ¡ salvo.

3. **Carregamento dos Arquivos CSV**:
   - LÃª e combina os arquivos CSV em um Ãºnico DataFrame usando `glob` e `pandas`.

4. **NormalizaÃ§Ã£o dos Dados**:
   - Padroniza os nomes das colunas (para minÃºsculas e remove espaÃ§os extras).
   - Converte tipos de dados, como IDs e quantidades para inteiros.
   - Trata a coluna de data para garantir o formato correto.

5. **OrganizaÃ§Ã£o e ExportaÃ§Ã£o**:
   - Ordena os dados e realiza diversas transformaÃ§Ãµes, como remoÃ§Ã£o de duplicatas e preenchimento de valores nulos.
   - Salva os dados transformados em um novo arquivo CSV.

### **2. Scripts de CriaÃ§Ã£o e Carregamento no Banco de Dados**

ApÃ³s a limpeza e transformaÃ§Ã£o dos dados, os dados sÃ£o carregados no banco de dados "datamart" para serem usados em anÃ¡lises e relatÃ³rios.

#### Exemplo do script `criar_carregar_nome_da_tabela.py`:

1. **ImportaÃ§Ã£o das Bibliotecas**:
   - `pandas`: Para manipulaÃ§Ã£o dos dados.
   - `sqlalchemy`: Para conexÃ£o com o banco de dados e inserÃ§Ã£o de dados.
   - `psycopg2`: Para conexÃ£o com PostgreSQL.

2. **Direcionamento dos Dados**:
   - Define o caminho para os dados processados.

3. **ConexÃ£o com o Banco de Dados**:
   - Conecta-se ao banco de dados "datamart" usando `SQLAlchemy`.

4. **Carregamento dos Dados**:
   - Carrega os dados processados e insere-os no banco de dados usando `pandas.DataFrame.to_sql`.

5. **FinalizaÃ§Ã£o**:
   - Exibe uma mensagem de sucesso ao final do carregamento.

### **3. Script de CriaÃ§Ã£o e AgregaÃ§Ã£o dos Dados - `criar_carregar_tabela_agregada_postgres.py`**

#### Objetivo:
Criar uma nova tabela agregada com:
- **Receita total por cliente**
- **NÃºmero total de transaÃ§Ãµes por cliente**
- **Produto mais comprado por cliente**

Carregar essa tabela agregada no banco de dados **datamart** e salvar os dados no formato **Parquet** na camada **trusted** para anÃ¡lises mais performÃ¡ticas.

#### ExplicaÃ§Ã£o do CÃ³digo:

1. **ImportaÃ§Ã£o das Bibliotecas**:
   - `pandas`: Para manipulaÃ§Ã£o dos dados.
   - `sqlalchemy`: Para conectar ao banco de dados PostgreSQL.
   - `psycopg2`: Para a conexÃ£o com PostgreSQL.

2. **Carregamento dos Dados**:
   - Os dados processados das tabelas `clientes`, `transacoes` e `produtos` sÃ£o carregados em DataFrames do `pandas`.

3. **AgregaÃ§Ã£o dos Dados**:
   - **Receita total por cliente**: Somamos o valor das transaÃ§Ãµes por `cliente_id`.
   - **NÃºmero total de transaÃ§Ãµes por cliente**: Contamos o nÃºmero de transaÃ§Ãµes por `cliente_id`.
   - **Produto mais comprado por cliente**: Contamos a quantidade de cada produto comprado por `cliente_id` e escolhemos o produto com a maior quantidade comprada.

4. **Mesclagem dos DataFrames**:
   - As tabelas agregadas sÃ£o mescladas em um Ãºnico DataFrame, utilizando o `cliente_id` como chave de junÃ§Ã£o.

5. **ConexÃ£o com o Banco de Dados**:
   - Utilizamos `SQLAlchemy` para conectar ao banco de dados **datamart** e carregamos os dados agregados na tabela **clientes_aggregados**.

6. **Salvar os Dados no Banco de Dados**:
   - O mÃ©todo `to_sql()` do `pandas` Ã© utilizado para criar a tabela no banco de dados e inserir os dados.

7. **Salvar os Dados no Formato Parquet**:
   - Os dados agregados sÃ£o salvos no formato **Parquet** na camada **trusted** para garantir uma anÃ¡lise mais performÃ¡tica em etapas futuras.

8. **Mensagem de ConclusÃ£o**:
   - ApÃ³s a execuÃ§Ã£o, o script exibe uma mensagem confirmando a criaÃ§Ã£o da tabela no banco e o arquivo Parquet.

Toda a validaÃ§Ã£o do pipeline de popular o banco de dados com dados limpos e normalizados foi realizada utilizando o script `teste.py`. Esse script foi utilizado para confirmar se os dados estavam sendo transformados corretamente antes de serem salvos no Data Lake e no PostgreSQL. 

A validaÃ§Ã£o envolveu garantir que as operaÃ§Ãµes de transformaÃ§Ã£o e limpeza estavam sendo aplicadas corretamente aos dados antes da sua carga nos sistemas de armazenamento. O processo de validaÃ§Ã£o foi fundamental para assegurar que os dados no Data Lake e no banco de dados PostgreSQL estivessem consistentes, precisos e prontos para uso em anÃ¡lises e consultas.

## Banco de Dados DataMart no PostgreSQL

Este banco de dados foi criado para armazenar os dados limpos, normalizados e deduplicados provenientes de diversas fontes, como transaÃ§Ãµes de vendas, clientes e produtos. O objetivo Ã© oferecer uma estrutura eficiente para anÃ¡lise de dados e tomada de decisÃµes.

### Estrutura do Banco de Dados

O banco de dados DataMart contÃ©m as seguintes tabelas:

- **clientes**: ContÃ©m as informaÃ§Ãµes sobre os clientes, como `id_cliente`, `nome_cliente`, `email`, `telefone`.
- **produtos**: Armazena dados sobre os produtos, incluindo `id_produto`, `nome_produto`, `categoria`, `preco`.
- **transacoes**: Registra as transaÃ§Ãµes realizadas pelos clientes, com campos como `id_transacao`, `id_cliente`, `id_produto`, `quantidade`, `data_transacao`.

As tabelas foram populadas atravÃ©s de pipelines com os scripts:
- **limpar_transformacao_nome_da_tabela.py**: ResponsÃ¡vel pela limpeza, normalizaÃ§Ã£o e deduplicaÃ§Ã£o dos dados antes de inseri-los nas tabelas.
- **criar_carregar_nome_da_tabela.py**: ResponsÃ¡vel por carregar os dados limpos nas tabelas correspondentes do banco de dados.

AlÃ©m disso, uma tabela agregada foi criada com os seguintes dados:
- **Receita total por cliente**
- **NÃºmero total de transaÃ§Ãµes por cliente**
- **Produto mais comprado por cliente**

Essas informaÃ§Ãµes sÃ£o salvas tambÃ©m no DataLake, na camada **Trust**, garantindo que os dados mais confiÃ¡veis estejam disponÃ­veis para futuras anÃ¡lises.

## Melhorias de Performance

Para otimizar a performance do banco de dados e garantir consultas rÃ¡pidas, foram criados os seguintes Ã­ndices nas tabelas:

```sql
CREATE INDEX idx_clientes_id ON clientes(id);
CREATE INDEX idx_clientes_nome_sobrenome ON clientes(nome_cliente);
CREATE INDEX idx_produtos_id ON produtos(id_produto);
CREATE INDEX idx_produtos_descricao ON produtos(nome_produto);
CREATE INDEX idx_transacoes_id_cliente ON transacoes(id_cliente);
CREATE INDEX idx_transacoes_id_produto ON transacoes(id_produto);
CREATE INDEX idx_transacoes_data_transacao ON transacoes(data_transacao);

No projeto de Data Lake e DataMart descrito, a combinaÃ§Ã£o de
Ã­ndices eficientes, particionamento de tabelas, consultas agregadas otimizadas,
uso de tabelas materializadas e anÃ¡lise de planos de execuÃ§Ã£o Ã© fundamental para garantir que
o sistema seja capaz de lidar com grandes volumes de dados de maneira eficiente e rÃ¡pida.
A implementaÃ§Ã£o dessas boas prÃ¡ticas no PostgreSQL** pode resultar em uma performance
significativamente melhorada, mesmo Ã  medida que os dados aumentam.

Para facilitar a obtenÃ§Ã£o de informaÃ§Ãµes rÃ¡pidas e importantes para as regras de negÃ³cio, algumas consultas SQL foram criadas:

### NÃºmero de clientes ativos (Ãºltimos 3 meses):

```sql
SELECT 
    COUNT(DISTINCT t.id_cliente) AS clientes_ativos
FROM 
    transacoes t
WHERE 
    t.data_transacao >= CURRENT_DATE - INTERVAL '3 months';

### Receita total por produto:

```sql
SELECT p.id_produto, SUM(t.quantidade * p.preco) AS receita_total
FROM transacoes t
JOIN produtos p ON t.id_produto = p.id_produto
GROUP BY p.id_produto
ORDER BY receita_total DESC;

### Top 5 produtos mais vendidos (ano de 2024):

```sql
SELECT p.nome_produto, SUM(t.quantidade) AS total_vendido
FROM transacoes t
JOIN produtos p ON t.id_produto = p.id_produto
WHERE t.data_transacao BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY p.nome_produto
ORDER BY total_vendido DESC
LIMIT 5;


Essas consultas foram projetadas para fornecer informaÃ§Ãµes rÃ¡pidas e detalhadas sobre o desempenho de vendas, clientes ativos e os produtos mais vendidos, ajudando a orientar as decisÃµes de negÃ³cio.

##ConclusÃ£o

O banco de dados DataMart no PostgreSQL foi estruturado para oferecer dados limpos e otimizados, com foco na eficiÃªncia e na rapidez de acesso. As operaÃ§Ãµes de transformaÃ§Ã£o e carga foram automatizadas com pipelines, e as consultas analÃ­ticas fornecem insights valiosos para a empresa.

O arquivo do banco de dados foi particionado devido ao seu tamanho total de 45.233kB, o que dificultaria o upload no GitHub. Para contornar essa limitaÃ§Ã£o, foi utilizado o mÃ©todo `split` para dividir o arquivo em 3 partes:

- `parte_1aa`
- `parte_1ab`
- `parte_1ac`

O comando utilizado para dividir o arquivo foi:

```bash
split -b 22m seu_arquivo.sql parte_1

Para unir as partes do arquivo de volta, foi utilizado o comando:

```bash
cat parte_1aa parte_1ab parte_1ac > arquivo_completo.sql

##Esse processo foi realizado utilizando o Git Bash para facilitar o manuseio dos arquivos grandes, garantindo que o arquivo completo fosse reconstruÃ­do e pronto para ser utilizado.





